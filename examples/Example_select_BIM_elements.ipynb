{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT BIM ELEMENTS\n",
    "In this notebook, we select specific elements and classes from an IFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#IMPORT PACKAGES\n",
    "from rdflib import Graph, URIRef\n",
    "import os.path\n",
    "import importlib\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import open3d as o3d\n",
    "import uuid    \n",
    "import pye57 \n",
    "import ifcopenshell\n",
    "import ifcopenshell.geom as geom\n",
    "import ifcopenshell.util\n",
    "from ifcopenshell.util.selector import Selector\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import cv2\n",
    "\n",
    "#IMPORT MODULES\n",
    "from context import geomapi \n",
    "from geomapi.nodes import *\n",
    "import geomapi.utils as ut\n",
    "from geomapi.utils import geometryutils as gmu\n",
    "import geomapi.tools as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INITIALIZE SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS\n",
    "projectPath= os.path.join(\"D:\\\\Data\\\\2023-01 Colosseum\")\n",
    "sessionPath = os.path.join(projectPath,\"Research\")\n",
    "\n",
    "#BIM\n",
    "bimPath=os.path.join(sessionPath,\"BIM\")\n",
    "ifcPath=os.path.join(sessionPath,'BIM','RMCO_S01phases.ifc')\n",
    "\n",
    "bimGraphPath = os.path.join(sessionPath,\"BIM\",\"bimGraph.ttl\")\n",
    "bimClassesGraphPath = os.path.join(sessionPath,\"BIM\",\"bimClassesGraph.ttl\")\n",
    "bimClasses= {'COLONNA':['COLONNA','PARASTA'],\n",
    "            'PILASTRO':'PILASTRO',\n",
    "            'TRABEAZIONE':'TRABEAZIONE',\n",
    "            'ARCO':'ARCO',\n",
    "            'AGGETTO':'aggetto',\n",
    "            'FACADE':['CORNICE','MUR']}\n",
    "bimSeperatorGraphPath = os.path.join(sessionPath,\"BIM\",\"bimSeperatorGraph.ttl\")\n",
    "bimSeperators= {'LV-03_A':'3saOMnutrFHPtEwspUjESB',\n",
    "                'Facade_1':'0H$kAf94TFru8CbHUTFppg',\n",
    "                'LV-02.1_A':'3saOMnutrFHPtEwspUjEN8',\n",
    "                'LV-02_A':'3saOMnutrFHPtEwspUjEHi',\n",
    "                'LV-01_A':'3saOMnutrFHPtEwspUjEJs',\n",
    "                'LV-02_B':'3saOMnutrFHPtEwspUjEjK',\n",
    "                'LV-01_B': '3saOMnutrFHPtEwspUjEZ6',\n",
    "                'LV-01_C': '3saOMnutrFHPtEwspUjEXN'}\n",
    "bimTransform=np.array([[1.0,0.0, 0.0, -2.3118e+06  ], \n",
    "                [0.0, 1.0, 0.0, -4.6406e+06],\n",
    "                [0.0, 0.0, 1.0 ,0],\n",
    "                [0.0 ,0.0, 0.0, 1.000000000000]])\n",
    "bimMeshExportFolder=os.path.join(os.path.join(sessionPath,'MESH'))\n",
    "os.mkdir(bimMeshExportFolder) if not os.path.exists(bimMeshExportFolder) else False  \n",
    "ClassPointCloudsFolder=os.path.join(os.path.join(sessionPath,'ClassPointClouds'))\n",
    "os.mkdir(ClassPointCloudsFolder) if not os.path.exists(ClassPointCloudsFolder) else False\n",
    "bimClassNames=[]\n",
    "for c in bimClasses.values():\n",
    "    c=ut.item_to_list(c)\n",
    "    c=' '.join(c)\n",
    "    bimClassNames.append(ut.validate_string(c))\n",
    "#PCD\n",
    "csvPath=os.path.join(projectPath,'PCD','COLOSSEO_UNITO_test3_indexed.csv')\n",
    "# csvOutputPath=os.path.join(sessionPath,'PCD','COLOSSEO_UNITO_test3_indexed.csv')\n",
    "chunksize=1000000\n",
    "threshold=0.1 #distance theshold for inliers\n",
    "lasPath=os.path.join(projectPath,'PCD','COLOSSEO_UNITO - Cloud.las')\n",
    "outputlasPath=os.path.join(projectPath,'PCD','COLOSSEO_UNITO - Cloud - classes.las')\n",
    "\n",
    "#MESH\n",
    "meshPath=os.path.join(sessionPath,'MESH','COLONNA_PARASTA.obj')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE INPUTS & CREATE NODES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import BIMSeperator geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bimSeperatorNodes=to.ifc_to_nodes_by_guids(ifcPath,bimSeperators.values(),offsetTransform=bimTransform)\n",
    "# for n in bimSeperatorNodes:\n",
    "#     n.cartesianTransform= bimTransform \n",
    "#     n.resource.transform(bimTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bimMesh=gmu.join_geometries([n.resource for n in bimNodes if n.resource is not None])\n",
    "# bimSeperatorMesh=gmu.join_geometries([n.resource for n in bimSeperatorNodes if n.resource is not None])\n",
    "# o3d.visualization.draw_geometries([bimMesh]+[bimSeperatorMesh])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fig.**: Images of the BIM section of the colosseum with (a) BIM elements and (b) the point cloud, (c) selected subgroups per model type and (d) segmented point cloud per subgroup.\n",
    "\n",
    "<img src=\"../docs/pics/colosseum/columns1.PNG\" width = \"20%\">\n",
    "<img src=\"../docs/pics/colosseum/columns2.PNG\" width = \"25%\">\n",
    "\n",
    "<img src=\"../docs/pics/colosseum/columns3.PNG\" width = \"23%\">\n",
    "<img src=\"../docs/pics/colosseum/columns4.PNG\" width = \"20%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT AND SEPERATE BIM INTO SUBGROUPS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the IFC Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 461 BIMNodes created!\n"
     ]
    }
   ],
   "source": [
    "bimNodes=to.ifc_to_nodes_multiprocessing(ifcPath,offsetTransform=bimTransform)\n",
    "for n in bimNodes:\n",
    "    n.cartesianTransform= bimTransform \n",
    "    n.resource.transform(bimTransform)\n",
    "print(f' {str(len(bimNodes))} BIMNodes created!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select BIM objects per class based on their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([['COLONNA', 'PARASTA'], 'PILASTRO', 'TRABEAZIONE', 'ARCO', 'aggetto', ['CORNICE', 'MUR']])\n"
     ]
    }
   ],
   "source": [
    "print(bimClasses.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,  65, ['COLONNA', 'PARASTA'] !\n",
      "1,  75, ['PILASTRO'] !\n",
      "2,  13, ['TRABEAZIONE'] !\n",
      "3,  61, ['ARCO'] !\n",
      "4,  19, ['aggetto'] !\n",
      "5,  137, ['CORNICE', 'MUR'] !\n"
     ]
    }
   ],
   "source": [
    "# make this exclusive\n",
    "\n",
    "nodeLists=[]\n",
    "geometries=[]\n",
    "for i,c,name in zip(range(len(bimClasses.values())),bimClasses.values(),bimClassNames):\n",
    "    #select nodes\n",
    "    c=ut.item_to_list(c)\n",
    "    nodeLists.append([n for n in bimNodes if any(name in n.name for name in c ) and n.resource is not None])\n",
    "    print(f'{i},  {len(nodeLists[i])}, {c} !')\n",
    "    #combine geometries\n",
    "    if nodeLists[i] is not None:\n",
    "        geometries.append(gmu.join_geometries([n.resource for n in nodeLists[i] if n.resource is not None]))\n",
    "        #export geometries     \n",
    "        o3d.io.write_triangle_mesh(os.path.join(bimMeshExportFolder,name+'.obj'),geometries[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create point cloud per mesh geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_clouds=[]\n",
    "for g in geometries:\n",
    "    area=g.get_surface_area()\n",
    "    count=int(area/(threshold*threshold))\n",
    "    ref_clouds.append(g.sample_points_uniformly(number_of_points=count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) read sample mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh=o3d.io.read_triangle_mesh(meshPath)\n",
    "k = round(mesh.get_surface_area() * 1000)\n",
    "ref_clouds = [mesh.sample_points_uniformly(number_of_points = k, use_triangle_normal=True)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the BIM Classes in different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "coloredGeometries=[g.paint_uniform_color(ut.random_color()) for g in geometries if g is not None]\n",
    "o3d.visualization.draw_geometries(coloredGeometries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEGMENT PCD WITH BIMCLASSES USING PYLAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Las data (1.5 min for 110M points, requires 13Gb RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "las  = laspy.read(lasPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute nearest neighbors. Fast but operates on single core (10min for 110M points, 13Gb RAM required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_cloud,ref_arr=gmu.create_identity_point_cloud(ref_clouds)\n",
    "query_points=gmu.transform_points( las.xyz,bimTransform)\n",
    "indices, distances=gmu.compute_nearest_neighbors(np.asarray(ref_cloud.points), query_points)\n",
    "index=ref_arr[indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign nearest index as an extra dimension in the las file. (query points take 4Gb with 110M points so put it in function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points=None\n",
    "distances=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LasHeader(1.2, <PointFormat(2, 25 bytes of extra dims)>)>\n",
      "['X', 'Y', 'Z', 'intensity', 'return_number', 'number_of_returns', 'scan_direction_flag', 'edge_of_flight_line', 'classification', 'synthetic', 'key_point', 'withheld', 'scan_angle_rank', 'user_data', 'point_source_id', 'red', 'green', 'blue', '01 Materiali', '03 D', '02 TC', 'bimClass']\n",
      "[1 1 1 ... 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "las.add_extra_dim(laspy.ExtraBytesParams(name=\"bimClass\", type=\"uint8\"))\n",
    "las.bimClass=index[:,0]\n",
    "print(las.header)\n",
    "print(list(las.point_format.dimension_names))\n",
    "print(las['bimClass'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export las file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "las.write(outputlasPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fig.**: Images of the segmented point cloud with (a) Point cloud enirched with bim class as a feature (b) the ref_clouds from the BIM model, (c) the initial point cloud.\n",
    "\n",
    "<img src=\"../docs/pics/colosseum/columns5.PNG\" width = \"17%\">\n",
    "<img src=\"../docs/pics/colosseum/columns6.PNG\" width = \"23%\">\n",
    "<img src=\"../docs/pics/colosseum/columns7.PNG\" width = \"23%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEGMENT PCD AS DATAFRAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Without optimization (fast but for small point clouds e.g. 10-20M points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "df = pd.read_csv(csvPath,\n",
    "    sep= ' ',\n",
    "    header=0,  \n",
    "    names=[\"x\",\"y\",\"z\",\"R\", \"G\", \"B\", \"M\", \"TC\", \"Nx\", \"Ny\", \"Nz\" ])\n",
    "arr=np.zeros(len(df))\n",
    "pcd=gmu.dataframe_to_pcd(df)\n",
    "# pcd.transform(bimTransform)\n",
    "#compute distance to identityPointCloud   \n",
    "for i,ref_cloud in enumerate(ref_clouds):\n",
    "    time\n",
    "    distances=pcd.compute_point_cloud_distance(ref_cloud)\n",
    "    #select indices within a distance threshold\n",
    "    indices=np.where(np.asarray(distances) <= threshold)[0]\n",
    "    np.put(arr, indices, i)\n",
    "# assign new column and export df\n",
    "df = df.assign(className=arr)\n",
    "\n",
    "# df['class'] = arr.tolist()\n",
    "df.to_csv(csvPath,mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,ref_cloud,name in zip(range(len(bimClasses.values())),ref_clouds,bimClassNames):\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chunked without optimization (slow but memory proof for medium point clouds e.g. 20-100M points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks  = pd.read_csv(csvPath,\n",
    "    sep= ' ',\n",
    "    header=0,  \n",
    "    names=[\"x\",\"y\",\"z\",\"R\", \"G\", \"B\", \"M\", \"TC\", \"Nx\", \"Ny\", \"Nz\" ],\n",
    "    chunksize=chunksize,\n",
    "    iterator=True)\n",
    "for chunk in chunks: \n",
    "    # create integer based array for the classes\n",
    "    arr=np.zeros(len(chunk))   \n",
    "    #create point cloud\n",
    "    pcd=gmu.dataframe_to_pcd(chunk)\n",
    "    #transform to local coordinate system\n",
    "    pcd.transform(bimTransform)\n",
    "    #compute distance to identityPointCloud    \n",
    "    for i,ref_cloud in enumerate(ref_clouds):\n",
    "        distances=pcd.compute_point_cloud_distance(ref_cloud)\n",
    "        #select indices within a distance threshold       \n",
    "        ind=np.where(np.asarray(distances) <= threshold)[0]\n",
    "        np.put(arr, indices, i)\n",
    "    # assign new column and export df\n",
    "    df['class'] = arr  \n",
    "    chunk.to_csv(csvPath, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#  # this is some unused code to iteratively write data to csv file\n",
    "#   test1=chunk.iloc[ind]\n",
    "#             # #export point clouds\n",
    "#             with open(os.path.join(ClassPointCloudsFolder,name+'.csv'), \"a\") as csv:\n",
    "#                 test1.to_csv(csv,mode='a')\n",
    "#             print(f'{len(test1)} of {chunksize} exported.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DASK multiprocessing optimization (fast and memory proof for large point clouds e.g. >100M points). Note that this is three times slower for small point clouds due to working spawning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 //X             Y          Z    R    G   B  01_Materiali  \\\n",
      "0       2.311879e+06  4.640674e+06  32.195537   74   67  58           0.0   \n",
      "1       2.311879e+06  4.640674e+06  32.184521   90   84  73           0.0   \n",
      "2       2.311879e+06  4.640674e+06  32.204296  117  102  89           0.0   \n",
      "3       2.311879e+06  4.640674e+06  32.212200  116  106  91           0.0   \n",
      "4       2.311879e+06  4.640673e+06  32.135959   79   68  57           0.0   \n",
      "...              ...           ...        ...  ...  ...  ..           ...   \n",
      "571611  2.311875e+06  4.640673e+06  35.283402   46   54  49           0.0   \n",
      "571612  2.311875e+06  4.640673e+06  35.177899   80   80  81           0.0   \n",
      "571613  2.311875e+06  4.640673e+06  35.307801   44   51  48           0.0   \n",
      "571614  2.311875e+06  4.640673e+06  35.254101   96  101  98           0.0   \n",
      "571615  2.311875e+06  4.640673e+06  35.325299   87   88  90           0.0   \n",
      "\n",
      "        03_D  02_TC  Original_cloud_index        Nx        Ny        Nz  \\\n",
      "0        1.0    0.0                   0.0  0.120092  0.920706  0.371319   \n",
      "1        1.0    0.0                   0.0 -0.091646  0.994860 -0.043063   \n",
      "2        1.0    0.0                   0.0  0.882805  0.443570  0.154599   \n",
      "3        1.0    0.0                   0.0  0.601004 -0.043633 -0.798054   \n",
      "4        1.0    0.0                   0.0  0.866770  0.012094 -0.498561   \n",
      "...      ...    ...                   ...       ...       ...       ...   \n",
      "571611   2.0    0.0                   3.0 -0.869342 -0.397370 -0.293841   \n",
      "571612   2.0    0.0                   3.0 -0.910107  0.413445  0.027739   \n",
      "571613   2.0    0.0                   3.0 -0.030668 -0.965433 -0.258839   \n",
      "571614   2.0    0.0                   3.0 -0.532037 -0.806637 -0.257437   \n",
      "571615   2.0    0.0                   3.0 -0.531527  0.014953 -0.846909   \n",
      "\n",
      "        distance_to_ref  \n",
      "0          5.184515e+06  \n",
      "1          5.184516e+06  \n",
      "2          5.184515e+06  \n",
      "3          5.184515e+06  \n",
      "4          5.184515e+06  \n",
      "...                 ...  \n",
      "571611     5.184513e+06  \n",
      "571612     5.184513e+06  \n",
      "571613     5.184513e+06  \n",
      "571614     5.184513e+06  \n",
      "571615     5.184513e+06  \n",
      "\n",
      "[3424050 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(csvPath,\n",
    "                 header=0, \n",
    "                sep= ' ')\n",
    "\n",
    "\n",
    "def create_and_compute_distance(partition, ref_clouds):\n",
    "    points=partition[partition.columns[:3]].values    \n",
    "    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(points))\n",
    "    for i,ref_cloud,name in zip(range(len(bimClasses.values())),ref_clouds,bimClassNames):\n",
    "        distances=pcd.compute_point_cloud_distance(ref_cloud)\n",
    "        #select indices within a distance threshold       \n",
    "        ind=np.where(np.asarray(distances) <= threshold)[0]\n",
    "        np.put(arr, indices, i)\n",
    "    distance = pcd.compute_point_cloud_distance(ref_cloud)\n",
    "    \n",
    "    partition['distance_to_ref'] = distance\n",
    "    return partition\n",
    "\n",
    "def create_point_cloud_dask(df, ref_cloud):\n",
    "    point_cloud_partitions = df.map_partitions(create_and_compute_distance, ref_clouds)\n",
    "    return point_cloud_partitions.compute()\n",
    "\n",
    "result = create_point_cloud_dask(df, ref_clouds).to_parquet\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first dask attempt\n",
    "# #define a function\n",
    "# def compute_index(df):\n",
    "#     xyz=df.iloc[:,[0,1,2]]\n",
    "#     pcd=o3d.geometry.PointCloud()\n",
    "#     pcd.points=o3d.utility.Vector3dVector(xyz.to_numpy())\n",
    "#     # pcd=gmu.dataframe_to_pcd(df)\n",
    "#     return 10\n",
    "\n",
    "# #compute function as delayed function\n",
    "# lazy_results = []\n",
    "# for chunk in dask_dataframe: #test\n",
    "#     chunk=chunk.compute()\n",
    "#     lazy_result = dask.delayed(compute_index)(chunk)\n",
    "#     lazy_results.append(lazy_result)\n",
    "# # dask.compute(*lazy_results)\n",
    "# futures = dask.persist(*lazy_results)  # trigger computation in the background\n",
    "# client.cluster.scale(10)  # ask for ten 4-thread workers\n",
    "# results = dask.compute(*futures)\n",
    "# results[:5]\n",
    "\n",
    "\n",
    "# results = dask.compute(*futures)\n",
    "# results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this function does not work because you cannot jointly write in parallel in the same file\n",
    "# for chunk in dask_dataframe.partitions:    \n",
    "#     chunk=chunk.compute()\n",
    "#     # print(chunk[0])\n",
    "#     # xyz=chunk.get(['Y', 'Z'])\n",
    "\n",
    "#     pcd=gmu.dataframe_to_pcd(chunk)\n",
    "#     # xyz=chunk.\n",
    "#     # print(pcd)\n",
    "    \n",
    "\n",
    "#     # pcd=o3d.geometry.PointCloud()\n",
    "#     # pcd.points=o3d.utility.Vector3dVector(xyz.to_numpy())\n",
    "#     # #transform to local coordinate system\n",
    "#     pcd.transform(bimTransform)\n",
    "#     #compute distance to identityPointCloud    \n",
    "#     for bimpcd,name in zip(bimPointClouds,bimClassNames):\n",
    "#         distances=pcd.compute_point_cloud_distance(bimpcd)\n",
    "#         #remove distances > threshold\n",
    "#         ind=np.where(np.asarray(distances) <= threshold)[0]\n",
    "#         #select indices based on closest point        \n",
    "#         if ind.size >0:\n",
    "#             test1=chunk.iloc[ind]\n",
    "#             # #export point clouds\n",
    "#             with open(os.path.join(ClassPointCloudsFolder,name+'.csv'), \"a\") as csv:\n",
    "#                 test1.to_csv(csv,mode='a')\n",
    "#             print(f'{len(test1)} of {len(chunk)} exported.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) filter distance calculation based on geometry shape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_environment3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "801b4083378541fd050d6c91abf6ec053c863905e8162e031d57b83e7cdb3051"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
